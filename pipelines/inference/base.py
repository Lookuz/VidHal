import string
import re
import json
import torch
from tqdm import tqdm

from ...dataset import VidHalDataset
from ...utils import generate_display_order

class VidHalInferencePipeline:
    """
    VidHalInferencePipeline and it's derivatives should handle:
    1. Formatting of prompt to be provided to the model.
    2. Generation of response from the given prompt and video.
    """
    def __init__(
        self, 
        model,
        dataset : VidHalDataset,
        option_display_order : dict = None,
        generation_config = {},
        *args, **kwargs
    ):
        self.model = model
        self.dataset = dataset
        self.generation_config = generation_config
        self.option_display_order = option_display_order if option_display_order is not None else generate_display_order(dataset)

    def format_options_prompt(self, captions, video_id=None, option_to_rank=None):
        """
        Generates the sub-prompt containing line-break separated [option : caption] to be displayed to the model
        """
        assert option_to_rank is not None and video_id is not None # Either video ID provided to use pre-defined ordering, or custom option ordering must be provided
        if option_to_rank is None:
            option_to_rank = self.option_display_order[video_id]
        options_prompt = "\n".join([f"{option}. {captions[rank]}" for option, rank in option_to_rank.items()])

        return options_prompt

    def format_prompt(
        self, 
        main_prompt, 
        options_prompt, 
        system_prompt=None, 
        *args, **kwargs):
        """
        NOTE: Implement this according to your model requirements

        Expected return type:
            prompts (tuple): Consisting of (main_prompt, system_prompt). If only one prompt is used, system prompt can be left optionally empty
        """
        raise NotImplementedError
    
    def generate_response(
        self, 
        model, 
        video, 
        main_prompt, system_prompt=None,
        generation_config={},
        *args, **kwargs):
        """
        NOTE: Implement this according to your model requirements

        Expected return type:
            response (str) : Response generated by the model.
        """
        raise NotImplementedError
    
    def run(self, save_path=None):
        raise NotImplementedError

class VidHalMCQAInferencePipeline(VidHalInferencePipeline):
    system_prompt_instruction = "You are provided with a video and a set of several captions. " \
        "Your task is to watch the video provided carefully, and select the caption that best describes the video. " \
        "Provide your answer only as a single letter representing the option whose caption that best describes the video, without any explanation."
    main_prompt_instruction = "Watch the video provided, and choose the option whose caption describes the video most accurately."
    def __init__(self, model, dataset, generation_config={}, *args, **kwargs):
        super().__init__(model, dataset, generation_config, *args, **kwargs)

    def process_response(self, response):
        """
        Parses the generated response to extract only the selected option.
        """
        last_option = list(string.ascii_uppercase)[self.num_captions - 1]
        match = re.search(fr"\b[a-{last_option.lower()}A-{last_option}]\b", response)
        match = match.group(0).upper().strip(";:., ") if match else None

        return match if match else response # If no match, keep original response in case model replies with caption instead of option
    
    def run(self, save_path=None):
        responses = {}
        with torch.inference_mode(), torch.no_grad():
            for i in tqdm(range(len(self.dataset))):
                example = self.dataset[i]
                video, video_id, captions = example["video"], example["video_id"], example["captions"]

                # Format caption options to be displayed to the model
                options_prompt = self.format_options_prompt(video_id=video_id, captions=captions)
                main_prompt, system_prompt = self.format_prompt(
                    self.main_prompt_instruction, options_prompt, self.system_prompt_instruction
                )

                # Generate response from the model
                response = self.generate_response(
                    self.model, video, main_prompt=main_prompt, system_prompt=system_prompt, generation_config=self.generation_config
                )
                response = self.process_response(response)

                responses[video_id] = response

        if save_path is not None:
            with open(save_path, "r") as f:
                json.dump(responses, f, indent=4)

class VidHalRelativeCOInferencePipeline(VidHalMCQAInferencePipeline):
    def reorder_options(self, captions, option_to_rank):
        """
        Re-orders the option prefixes (A, B, C) if there are less then the total number of captions presented to the model
        (e.g. When using only 2 of the M captions for relative caption ordering)
        """
        if len(captions) >= self.num_captions:
            return option_to_rank

        # Get options only if they exist in the captions
        option_to_rank = {option : rank for option, rank in option_to_rank.items() if rank in captions}
        # Adjust option letters
        option_prefix = list(string.ascii_uppercase)[:len(captions)]
        option_to_rank =  {option_prefix[i] : rank for i, (_, rank) in enumerate(
            sorted(list(option_to_rank.items()), key=lambda x: x[0])
        )}

        return option_to_rank
    
    def prompt_paired_question(self, video, captions, options, option_to_rank):
        # Reorder keys (e.g. A, C -> A, B), track mapping
        display_options = list(string.ascii_uppercase)[:len(options)]
        remapped_option_to_rank = {display_options[i] : rank for i, (_, rank) in enumerate(sorted(list(option_to_rank.items(), lambda x : x[0])))}
        remapped_to_original = {display_options[i] : option for i, (option, _) in enumerate(sorted(list(option_to_rank.items(), lambda x : x[0])))}

        # Format prompt and generate response
        options_prompt = self.format_options_prompt(captions=captions, option_to_rank=remapped_option_to_rank)
        main_prompt, system_prompt = self.format_prompt(
            self.main_prompt_instruction, options_prompt, self.system_prompt_instruction
        )
        response = self.generate_response(
            self.model, video, main_prompt=main_prompt, system_prompt=system_prompt, generation_config=self.generation_config
        )

        # Process response and map back to original
        response = self.process_response(response)
        response = remapped_to_original[response]

        return response
    
    def prompt_relative_ordering(self, video, video_id, captions):
        order = []
        # Transform from rank -> caption to option -> caption
        option_to_rank = self.option_display_order[video_id]
        rank_to_option = {v : k for k, v in option_to_rank.items()}
        captions = {
            rank_to_option[rank] : caption for rank, caption in captions.items()
        }
        options = sorted(list(captions.keys()))
        for option_A, option_B in zip(options, options[1:]):
            response = self.prompt_paired_question(video, captions, [option_A, option_B], option_to_rank)

        return order

    def run(self, save_path=None):
        with torch.inference_mode(), torch.no_grad():
            for i in tqdm(range(len(self.dataset))):
                example = self.dataset[i]
                video, video_id, captions = example["video"], example["video_id"], example["captions"]
